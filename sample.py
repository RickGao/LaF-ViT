import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from dataset import UTKFaceDataset, val_transforms
from model import LaFViT
import matplotlib.pyplot as plt
import numpy as np
import os
import random

# ==========================================
# 1. æ ‡ç­¾æ˜ å°„å­—å…¸
# ==========================================
GENDER_MAP = {0: 'Male', 1: 'Female'}
RACE_MAP = {0: 'White', 1: 'Black', 2: 'Asian', 3: 'Indian', 4: 'Others'}


def parse_args():
    parser = argparse.ArgumentParser(description="Visualize Best Predictions from Validation Set (Individual Images)")
    parser.add_argument('--data_dir', type=str, default='./data/UTKFace', help='Dataset path')
    parser.add_argument('--model_path', type=str, required=True, help='Path to best checkpoint')
    parser.add_argument('--num_samples', type=int, default=6, help='Number of images to save')

    # éªŒè¯é›†åˆ’åˆ†å‚æ•° (å¿…é¡»å’Œè®­ç»ƒä¸€è‡´)
    parser.add_argument('--seed', type=int, default=42, help='Random seed for split')
    parser.add_argument('--val_percent', type=int, default=10, help='Validation split percentage (default: 10)')

    # è¾“å‡ºç›®å½• (é»˜è®¤å­˜åˆ° sample æ–‡ä»¶å¤¹)
    parser.add_argument('--output_dir', type=str, default='sample_individual', help='Output directory for individual images')

    return parser.parse_args()


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def denormalize(tensor):
    """è¿˜åŸå½’ä¸€åŒ–çš„å›¾ç‰‡ä»¥ä¾¿æ˜¾ç¤º (ImageNet Stats)"""
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img = tensor.permute(1, 2, 0).cpu().numpy()
    img = img * std + mean
    return np.clip(img, 0, 1)


def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --- 0. å‡†å¤‡è¾“å‡ºç›®å½• ---
    os.makedirs(args.output_dir, exist_ok=True)

    print(f"ğŸš€ Sampling Individual Best Predictions | Device: {device} | Seed: {args.seed}")
    print(f"ğŸ“‚ Output folder: {args.output_dir}")

    # --- 1. è®¾ç½®éšæœºç§å­ (è‡³å…³é‡è¦) ---
    set_seed(args.seed)

    # --- 2. åŠ è½½æ¨¡å‹ ---
    print(f"ğŸ§  Loading model from: {args.model_path}")
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„æ¨¡å‹ä¸éœ€è¦ use_hard å‚æ•°ï¼Œå¦‚æœéœ€è¦è¯·è‡ªè¡Œæ·»åŠ 
    model = LaFViT(pretrained=False)
    state_dict = torch.load(args.model_path, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()

    # --- 3. ä¸¥æ ¼å¤ç°éªŒè¯é›†åˆ’åˆ† ---
    print(f"ğŸ“Š Reconstructing Validation Set (Split: {args.val_percent}%)")
    full_ds = UTKFaceDataset(args.data_dir, transform=val_transforms)
    total_len = len(full_ds)

    # è®¡ç®—åˆ’åˆ†é•¿åº¦
    val_len = int(total_len * args.val_percent / 100)
    train_len = total_len - val_len

    # ä½¿ç”¨ generator ç¡®ä¿å’Œè®­ç»ƒæ—¶çš„éšæœºåˆ’åˆ†ä¸€æ¨¡ä¸€æ ·
    gen = torch.Generator().manual_seed(args.seed)
    _, val_subset = random_split(full_ds, [train_len, val_len], generator=gen)

    print(f"   -> Validation set size: {len(val_subset)} images")

    # Shuffle=True è¿™é‡Œæ˜¯ä¸ºäº†åœ¨éªŒè¯é›†é‡ŒéšæœºæŒ‘å›¾
    loader = DataLoader(val_subset, batch_size=32, shuffle=True, num_workers=2)

    # --- 4. å¯»æ‰¾â€œå®Œç¾â€æ ·æœ¬ ---
    print("ğŸ” Searching for high-quality predictions (Age Err < 3, Gender & Race Correct)...")
    best_samples = []

    with torch.no_grad():
        for batch in loader:
            imgs = batch['image'].to(device)
            ages = batch['age'].to(device)
            genders = batch['gender'].to(device)
            races = batch['race'].to(device)

            # æ¨ç†
            age_preds, g_logits, r_logits = model(imgs, stage="stage2")

            # è¿˜åŸæ•°å€¼
            pred_ages = age_preds.flatten() * 100.0
            pred_genders = torch.argmax(g_logits, dim=1)
            pred_races = torch.argmax(r_logits, dim=1)

            # éå† Batch
            for i in range(len(imgs)):
                if len(best_samples) >= args.num_samples:
                    break

                # ç­›é€‰æ¡ä»¶
                age_err = abs(pred_ages[i].item() - ages[i].item())
                g_correct = (pred_genders[i] == genders[i])
                r_correct = (pred_races[i] == races[i])

                # æŒ‘é€‰è¯¯å·®ç‰¹åˆ«å° (< 3å²) ä¸”åˆ†ç±»å…¨å¯¹çš„æ ·æœ¬
                if age_err < 3.0 and g_correct and r_correct:
                    best_samples.append({
                        'img': imgs[i].cpu(),
                        'gt_age': ages[i].item(),
                        'pred_age': pred_ages[i].item(),
                        'gt_gen': genders[i].item(),
                        'pred_gen': pred_genders[i].item(),
                        'gt_race': races[i].item(),
                        'pred_race': pred_races[i].item()
                    })

            if len(best_samples) >= args.num_samples:
                break

    # --- 5. ç‹¬ç«‹ç»˜å›¾ä¸ä¿å­˜ ---
    if not best_samples:
        print("âš ï¸ No perfect samples found in this batch. Try increasing error threshold or batch size.")
        return

    print(f"ğŸ¨ Saving {len(best_samples)} individual images to {args.output_dir}...")

    for idx, sample in enumerate(best_samples):
        # åˆ›å»ºä¸€ä¸ªæ–°çš„ç”»å¸ƒ
        plt.figure(figsize=(4, 4.5))

        # æ˜¾ç¤ºå›¾ç‰‡
        vis_img = denormalize(sample['img'])
        plt.imshow(vis_img)
        plt.axis('off')

        # å‡†å¤‡æ ‡ç­¾æ–‡å­—
        p_age = sample['pred_age']
        t_age = sample['gt_age']
        p_gen = GENDER_MAP[sample['pred_gen']]
        t_gen = GENDER_MAP[sample['gt_gen']]
        p_race = RACE_MAP[sample['pred_race']]
        t_race = RACE_MAP[sample['gt_race']]

        # æ„é€ æ–‡å­—ï¼šä¸Šé¢æ˜¯é¢„æµ‹å€¼(GT)ï¼Œä¸‹é¢æ˜¯äººå£å±æ€§
        title_text = (
            f"Pred: {p_age:.1f} (GT: {t_age:.0f})\n"
            f"{p_gen} | {p_race}"
        )

        # ç¾åŒ–æ–‡å­—æ¡†ï¼Œæ”¾åœ¨å›¾ç‰‡ä¸‹æ–¹