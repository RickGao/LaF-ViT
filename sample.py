import argparse
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from dataset import UTKFaceDataset, val_transforms
from model import LaFViT
import matplotlib.pyplot as plt
import numpy as np
import os
import random

# ==========================================
# 1. æ ‡ç­¾æ˜ å°„å­—å…¸
# ==========================================
GENDER_MAP = {0: 'Male', 1: 'Female'}
RACE_MAP = {0: 'White', 1: 'Black', 2: 'Asian', 3: 'Indian', 4: 'Others'}


def parse_args():
    parser = argparse.ArgumentParser(description="Visualize Random Predictions (No Filtering)")
    parser.add_argument('--data_dir', type=str, default='./data/UTKFace', help='Dataset path')
    parser.add_argument('--model_path', type=str, required=True, help='Path to best checkpoint')
    parser.add_argument('--num_samples', type=int, default=6, help='Number of images to save')

    # éªŒè¯é›†åˆ’åˆ†å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='Random seed for split')
    parser.add_argument('--val_percent', type=int, default=10, help='Validation split percentage')
    # è¾“å‡ºç›®å½•
    parser.add_argument('--output_dir', type=str, default='sample_random', help='Output directory')

    return parser.parse_args()


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def denormalize(tensor):
    """è¿˜åŽŸå½’ä¸€åŒ–çš„å›¾ç‰‡ä»¥ä¾¿æ˜¾ç¤º"""
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img = tensor.permute(1, 2, 0).cpu().numpy()
    img = img * std + mean
    return np.clip(img, 0, 1)


def main():
    args = parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --- 0. å‡†å¤‡è¾“å‡ºç›®å½• ---
    os.makedirs(args.output_dir, exist_ok=True)

    print(f"ðŸš€ Sampling Random Predictions | Device: {device} | Seed: {args.seed}")
    print(f"ðŸ“‚ Output folder: {args.output_dir}")

    # --- 1. è®¾ç½®éšæœºç§å­ ---
    set_seed(args.seed)

    # --- 2. åŠ è½½æ¨¡åž‹ ---
    print(f"ðŸ§  Loading model from: {args.model_path}")
    # åŠ ä¸Š use_hard å‚æ•°ä»¥å…¼å®¹ä½ çš„ Ablation æ¨¡åž‹
    model = LaFViT(pretrained=False)
    state_dict = torch.load(args.model_path, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()

    # --- 3. ä¸¥æ ¼å¤çŽ°éªŒè¯é›†åˆ’åˆ† ---
    print(f"ðŸ“Š Reconstructing Validation Set (Split: {args.val_percent}%)")
    full_ds = UTKFaceDataset(args.data_dir, transform=val_transforms)
    total_len = len(full_ds)

    val_len = int(total_len * args.val_percent / 100)
    train_len = total_len - val_len

    gen = torch.Generator().manual_seed(args.seed)
    _, val_subset = random_split(full_ds, [train_len, val_len], generator=gen)

    print(f"   -> Validation set size: {len(val_subset)} images")

    # Shuffle=True ä¿è¯éšæœºæ€§
    loader = DataLoader(val_subset, batch_size=32, shuffle=True, num_workers=2)

    # --- 4. æ”¶é›†æ ·æœ¬ (ä¸ç­›é€‰) ---
    print("ðŸ” Collecting random samples (showing both Correct and Incorrect predictions)...")
    samples = []

    with torch.no_grad():
        for batch in loader:
            imgs = batch['image'].to(device)
            ages = batch['age'].to(device)
            genders = batch['gender'].to(device)
            races = batch['race'].to(device)

            # æŽ¨ç†
            age_preds, g_logits, r_logits = model(imgs, stage="stage2")

            # è¿˜åŽŸæ•°å€¼
            pred_ages = age_preds.flatten() * 100.0
            pred_genders = torch.argmax(g_logits, dim=1)
            pred_races = torch.argmax(r_logits, dim=1)

            # éåŽ† Batch
            for i in range(len(imgs)):
                if len(samples) >= args.num_samples:
                    break

                # ç›´æŽ¥æ·»åŠ ï¼Œä¸å†åˆ¤æ–­ if correct
                samples.append({
                    'img': imgs[i].cpu(),
                    'gt_age': ages[i].item(),
                    'pred_age': pred_ages[i].item(),
                    'gt_gen': genders[i].item(),
                    'pred_gen': pred_genders[i].item(),
                    'gt_race': races[i].item(),
                    'pred_race': pred_races[i].item()
                })

            if len(samples) >= args.num_samples:
                break

    # --- 5. ç‹¬ç«‹ç»˜å›¾ä¸Žä¿å­˜ ---
    print(f"ðŸŽ¨ Saving {len(samples)} individual images to {args.output_dir}...")

    for idx, sample in enumerate(samples):
        plt.figure(figsize=(4, 5.0))  # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ç”»å¸ƒï¼Œç»™ä¸‰è¡Œæ–‡å­—ç•™ç©ºé—´

        # æ˜¾ç¤ºå›¾ç‰‡
        vis_img = denormalize(sample['img'])
        plt.imshow(vis_img)
        plt.axis('off')

        # å‡†å¤‡æ ‡ç­¾æ–‡å­—
        p_age = sample['pred_age']
        t_age = sample['gt_age']

        p_gen_str = GENDER_MAP[sample['pred_gen']]
        t_gen_str = GENDER_MAP[sample['gt_gen']]

        p_race_str = RACE_MAP[sample['pred_race']]
        t_race_str = RACE_MAP[sample['gt_race']]

        # æž„é€ è¯¦ç»†çš„ä¸‰è¡Œæ–‡å­—ï¼šPred vs GT
        title_text = (
            f"Age: {p_age:.1f} (GT: {t_age:.0f})\n"
            f"Gen: {p_gen_str} (GT: {t_gen_str})\n"
            f"Race: {p_race_str} (GT: {t_race_str})"
        )

        # æ ‡è®°é¢œè‰²ï¼šå¦‚æžœè¯¯å·®å¤§æˆ–è€…åˆ†ç±»é”™ï¼Œå¯ä»¥ç”¨çº¢è‰²è¾¹æ¡†(è¿™é‡Œç®€å•èµ·è§åªç”¨æ–‡å­—)
        # ç¾ŽåŒ–æ–‡å­—æ¡†
        plt.title(title_text, fontsize=11, fontweight='bold', pad=10,
                  bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', boxstyle='round,pad=0.3'))

        plt.tight_layout()

        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        filename = f"sample_{idx}_GT{t_age:.0f}_{t_gen_str}_{t_race_str}.png"
        save_path = os.path.join(args.output_dir, filename)

        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  -> Saved: {filename}")

    print(f"âœ… Done!")


if __name__ == "__main__":
    main()